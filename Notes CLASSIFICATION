CLASSIFICATION
=====================
Here u classofy shit unlike regression where you predict shit

1.Logistic Regression
2.K-Nearest Neighbors (K-NN)
3.Support Vector Machine (SVM)
4.Kernel SVM
5.Naive Bayes
6.Decision Tree Classification
7.Random Forest Classification


1. Logistic Regression
=======================
If we apply pure linear regression we mess things up a bit
so what we do to the linear regression formula i.e
generally used to predict binary classifiction

y=b0+b1*x1
we apply sigmoid fucntion
p = 1/(1+e**-y)

and now 1+e**-y=1/p
e**-y=1-p/p
y=ln(p/(1+p))=b0+b1*x1 ---> this is best fitting line in the dataset

this makes the graph to predict more beautiful
with this we can probability --> probability can is denoted by p^----> In ML ^ denotes predicted value

once the probabilty is predicted from the select tresholod p^ and then if <then p^ then into NO group
                                                                       if > the p^ then into YES group
========================
K-Nearest Negib

--> the data set is divided into two parts two neighbours
--> we have to classify if the new data point where among the nigbhour fall


Step 1: Choose the number K of neighbors
Step 2: take the K nearest neighbors of the new data point, according to the euclidean distance
Step 3: Among these K neighbors, count the number of data points in each category
Step 4: Assign the new data point to the category where you counted the most neighbors

---Model is ready

eucledian distance --> {(x1-x2)^2 + (y1-y2)^2} ^0.5

============================================================================
SVM --> Support vector machine

Derive a line to separate the neighbours...or here the points
The line is chosen such that the distance between the points is marginalized
and the two closest points are called SUPPORT VECTORS

This line is called Maximum Margin Hyperplane (Maximum Margin Classifier)

below --> negative hyper plan
above  --> Positive hyper plane


==============================
Kernel -SVM

What if ur life is so misarable and u end up having a data set that is non-linearly separable! oh yea! your fucked up mate!
so for that u use kernel-SVM!

Non-linear data set --> build decision tree by increasing dimensionality --> then get it back
many  mapping functions exist
Woah! u have no clue! what i have just gone thru!!check the internet and get surprosed!!

A simple method is to project in 3D and the plane separting the points will be Hyperplane!
Mapping to higher dimension generally takes a lot of computation power
so this is slow as a snail

so we have to change the approach --> this is called kernel methdo

THE GUASSIAN RBF KERNEL
==========================

I really cant make computer notes for this section! Check it out in wikipedia...its to god like to miss

Sigmoig Function
==============================
explained in the notes somewhere!

Polynomial Kernels
====================
cool stuff check internet for more pictorial stuff it will help you understand


=====================
Naive Bayes

P(A|B)= P(B|A) * P(A) / P(B)

machin1 30 pre hour
machin2 20 per hour

out of all produced 1% are defective
out of all defective 50% are from machin1

what is the probability that a part produced by mach2 is defective

Soltion -->
 total per hour 50 p(machine1)=30/50=0.6 p(machine2)=20/50=0.4
 P(defective)=1%

 p(machine1| defective)=50%
 p(machine2 | defective) =50%


P(defect | Machine2)  = P(machine2|defective) * p(defective) /p(machine2)
                      = 0.5 * 0.01/ 0.4
                       =1.25%





In this classification calculate probability and based on the probability we calssify
========================

Decision tree Classification!

The data set is splits into leafs
the split is made such that certain entry is maximized that is it tries to minimize entropy or the randomness

simillar to decisoion tree regression
============================
Random Forest Classification

Step1 : pick random K points from the training set
step 2 : Build the decision tree associated to there  K data points
step 3 : Choose the number of Ntree of trees you want to build and repeat step 1&2
step 4 : For a new data point make each one of your Ntree trees predict the cateogory
to which the data points belongs and assign the new data point to the category that wins the majority vote


======

CAP cover from slides
Cummalative accuracy profile

