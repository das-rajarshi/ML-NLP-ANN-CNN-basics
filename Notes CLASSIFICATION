CLASSIFICATION
=====================
Here u classofy shit unlike regression where you predict shit

1.Logistic Regression
2.K-Nearest Neighbors (K-NN)
3.Support Vector Machine (SVM)
4.Kernel SVM
5.Naive Bayes
6.Decision Tree Classification
7.Random Forest Classification


1. Logistic Regression
=======================
If we apply pure linear regression we mess things up a bit
so what we do to the linear regression formula i.e
generally used to predict binary classifiction

y=b0+b1*x1
we apply sigmoid fucntion
p = 1/(1+e**-y)

and now 1+e**-y=1/p
e**-y=1-p/p
y=ln(p/(1+p))=b0+b1*x1 ---> this is best fitting line in the dataset

this makes the graph to predict more beautiful
with this we can probability --> probability can is denoted by p^----> In ML ^ denotes predicted value

once the probabilty is predicted from the select tresholod p^ and then if <then p^ then into NO group
                                                                       if > the p^ then into YES group
========================
K-Nearest Negib

--> the data set is divided into two parts two neighbours
--> we have to classify if the new data point where among the nigbhour fall


Step 1: Choose the number K of neighbors
Step 2: take the K nearest neighbors of the new data point, according to the euclidean distance
Step 3: Among these K neighbors, count the number of data points in each category
Step 4: Assign the new data point to the category where you counted the most neighbors

---Model is ready

eucledian distance --> {(x1-x2)^2 + (y1-y2)^2} ^0.5

============================================================================
SVM --> Support vector machine

Derive a line to separate the neighbours...or here the points
The line is chosen such that the distance between the points is marginalized
and the two closest points are called SUPPORT VECTORS

This line is called Maximum Margin Hyperplane (Maximum Margin Classifier)

below --> negative hyper plan
above  --> Positive hyper plane


==============================
Kernel -SVM

What if ur life is so misarable and u end up having a data set that is non-linearly separable! oh yea! your fucked up mate!
so for that u use kernel-SVM!

Non-linear data set --> build decision tree by increasing dimensionality --> then get it back
many  mapping functions exist
Woah! u have no clue! what i have just gone thru!!check the internet and get surprosed!!

A simple method is to project in 3D and the plane separting the points will be Hyperplane!
Mapping to higher dimension generally takes a lot of computation power
so this is slow as a snail

so we have to change the approach --> this is called kernel methdo

THE GUASSIAN RBF KERNEL
==========================

I really cant make computer notes for this section! Check it out in wikipedia...its to god like to miss

Sigmoig Function
==============================
explained in the notes somewhere!

Polynomial Kernels
====================
cool stuff check internet for more pictorial stuff it will help you understand











