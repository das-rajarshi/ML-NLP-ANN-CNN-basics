Simple Linear Regression formula
--------------------------------
y=b0 +b1*x1
y=a+bx
y=dependent variable --> That is something you explain
                      --> like salary based on years
x=independent variable--> That is number of years which is needed to get salary(prediction)
b=factor

Salary VS Expereience
---------------------
1)plot

2)Regression
------------------------
a---> y-intercpt--> here salary for a fresher

salary=a+b*expereince -->This is a straight line

now we have to get the best fitting line which passes through maximum points

actual salary yi
and modeled salary y'i

calc --> sum of (yi-y'i)**2

To get the best fitting line --> has the minum sum of (yi-y'i)**2


======================================================================
Multiple Linear Regressions

y=a+ SUM(bixi)

y=dependent variable
b's are the inependent variable

Assumption -- Linearity | Homoscedasticity | Multovariate of errors | Independence of error | Lack of multicollinearity
These assumptions must be true before building a linear regression model
so check the data with the assumptions and then only build the regression

Dummy Variables
-----------------
In the data set Profit is a dependent variable and we have lots of independent variables
In our data set we state of the start-up it is a categorical variable
so our approch should be using dummy variables
make it a thumb rule that incase of categorical variables use dummy variables (remeber we used dummy vatiables in our template)

Categoriacal Dummy variables act as a light switch like if 1 then new york
                                                           0 then calafornia

Dummy Variable Trap
-----------------------------------
NOTE ALWAYS OMIT ONE DUMMY VARIABLE from the dummy variable set


BUILDING MODELS --> STEP-BY-STEP
-------------------------------------
Shit is easy if one dependent and one independent
But these days we have more than one independent to predict dependent

We are cheeky we throw some shit out (i.e we throw some independent variables out)
Take only dominating variables (independent) only those which play a role in predicting the independent

5-Thumb Methods
--------------------------------------
1. All-in
*2. Backward Elimination
*3. Forward Selection
*4. Bidirectional Elimination
5. Score Comparison


2-3-4 ---> are called STEP-WISE-REGRESSION
generally 4--> is called step-wise-regression

1. All-In throw all the variables in to predict
            -->Prior Knowledge
            --> You are forced to ( when you boss is an asshhole)
            --> Preparing for a backward elimination


2. Backward Eliminatin

step 1----> Select significane level to stay in the model (SL=0.05 example)
step 2-----> Fit the full  model with all possible predictors
step 3------> consider the predictor with the highest P-value. if P>SL, go to STEP 4. otherwise YOUR MODEL IS READY
step 4------> Remove the predictor
step 5------> Fit model with this variable (recurse with step 3)





P-Value is basically the error you want to tolerate.
For example you are saying I am ready to accept the null hypothesis with 95% accuracy level that means you are to ready for 5% error.

For eg: If you are performing some task like Basketball shots,
In this case 5% error means that you are ready to tolerate 5 miss out of 100 shots i.e your accuracy is 95,
you will shots 95 times accurately & will miss 5 balls


SL ---> it is a threshold against which we compare p-values.




3. Forward Selection

step 1: select a significance level to enter the model (e.g SL=0.05)
step 2: Fit all simple regression models y-xn select the one with the lowest P-value
step 3: Keep this variable and fit all possible models with one extra prerdicor added to the ones you already have
step 4: consider the predictor with the lowesr P-value if P<sL got to step 3, otherwise go to FINISH (Keep the previous Model and throw the current model)


4. Biredercitonal moel

Step 1: Select a significae leve to enter and to stay in the model
        e.g SLENTER = 0.05 , SLSTAY =0.05
STEP 2 : Perform the next step of Forward selection (new variables must have P<SLENTER to enter)
step 3 : Perform all steps of backward elimination (old variables must have P<SLSTAY to stay)
step 4 : No new variables can enter and no old variables can exit
            =----> Your model is ready


5. All Possible models --> Best model but resourse comparing

Step 1 : Select a criterion of goodness og fit
Step 2 : construct all possible regression models --> 2**N - 1 total combinations
Step 3 : Select the one with best criterion
        =---> Your model is ready



What we will be doing in the implementation part is using backward elimination methof --> Since it is quick! ROCK n ROll baby!




