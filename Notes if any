Simple Linear Regression formula
--------------------------------
y=b0 +b1*x1
y=a+bx
y=dependent variable --> That is something you explain
                      --> like salary based on years
x=independent variable--> That is number of years which is needed to get salary(prediction)
b=factor

Salary VS Expereience
---------------------
1)plot

2)Regression
------------------------
a---> y-intercpt--> here salary for a fresher

salary=a+b*expereince -->This is a straight line

now we have to get the best fitting line which passes through maximum points

actual salary yi
and modeled salary y'i

calc --> sum of (yi-y'i)**2

To get the best fitting line --> has the minum sum of (yi-y'i)**2


======================================================================
Multiple Linear Regressions

y=a+ SUM(bixi)

y=dependent variable
b's are the inependent variable

Assumption -- Linearity | Homoscedasticity | Multovariate of errors | Independence of error | Lack of multicollinearity
These assumptions must be true before building a linear regression model
so check the data with the assumptions and then only build the regression

Dummy Variables
-----------------
In the data set Profit is a dependent variable and we have lots of independent variables
In our data set we state of the start-up it is a categorical variable
so our approch should be using dummy variables
make it a thumb rule that incase of categorical variables use dummy variables (remeber we used dummy vatiables in our template)

Categoriacal Dummy variables act as a light switch like if 1 then new york
                                                           0 then calafornia

Dummy Variable Trap
-----------------------------------
NOTE ALWAYS OMIT ONE DUMMY VARIABLE from the dummy variable set


BUILDING MODELS --> STEP-BY-STEP
-------------------------------------
Shit is easy if one dependent and one independent
But these days we have more than one independent to predict dependent

We are cheeky we throw some shit out (i.e we throw some independent variables out)
Take only dominating variables (independent) only those which play a role in predicting the independent

5-Thumb Methods
--------------------------------------
1. All-in
*2. Backward Elimination
*3. Forward Selection
*4. Bidirectional Elimination
5. Score Comparison


2-3-4 ---> are called STEP-WISE-REGRESSION
generally 4--> is called step-wise-regression

1. All-In throw all the variables in to predict
            -->Prior Knowledge
            --> You are forced to ( when you boss is an asshhole)
            --> Preparing for a backward elimination


2. Backward Eliminatin

step 1----> Select significane level to stay in the model (SL=0.05 example)
step 2-----> Fit the full  model with all possible predictors
step 3------> consider the predictor with the highest P-value. if P>SL, go to STEP 4. otherwise YOUR MODEL IS READY
step 4------> Remove the predictor
step 5------> Fit model with this variable (recurse with step 3)





P-Value is basically the error you want to tolerate.
For example you are saying I am ready to accept the null hypothesis with 95% accuracy level that means you are to ready for 5% error.

For eg: If you are performing some task like Basketball shots,
In this case 5% error means that you are ready to tolerate 5 miss out of 100 shots i.e your accuracy is 95,
you will shots 95 times accurately & will miss 5 balls


SL ---> it is a threshold against which we compare p-values.




3. Forward Selection

step 1: select a significance level to enter the model (e.g SL=0.05)
step 2: Fit all simple regression models y-xn select the one with the lowest P-value
step 3: Keep this variable and fit all possible models with one extra prerdicor added to the ones you already have
step 4: consider the predictor with the lowesr P-value if P<sL got to step 3, otherwise go to FINISH (Keep the previous Model and throw the current model)


4. Biredercitonal moel

Step 1: Select a significae leve to enter and to stay in the model
        e.g SLENTER = 0.05 , SLSTAY =0.05
STEP 2 : Perform the next step of Forward selection (new variables must have P<SLENTER to enter)
step 3 : Perform all steps of backward elimination (old variables must have P<SLSTAY to stay)
step 4 : No new variables can enter and no old variables can exit
            =----> Your model is ready


5. All Possible models --> Best model but resourse comparing

Step 1 : Select a criterion of goodness og fit
Step 2 : construct all possible regression models --> 2**N - 1 total combinations
Step 3 : Select the one with best criterion
        =---> Your model is ready



What we will be doing in the implementation part is using backward elimination methof --> Since it is quick! ROCK n ROll baby!




=====================================================================================================
Polynomial Linear Regression
-----------------------------
y=b0 + sum(bn*(xn**n))

->this will help if the growth in not a linear equation but if it has some quadrature
->this is super when u want to predict things which grow exponentially
-> Then why is it still Linear?
-> Linear always refers to the coeffcients.


=======================================================================
Decision Tree Intuition

CART --:> Classification Tree
     --:> Regression Trees

x1 x2 y ---> Three values we dont need to actually see y

y--> dependent variable which we have to predict

We will work with only x1 | x2
-> The Scatter Plot will be split based on some conditions determined by the algo
-> The splits are based on mathematical entropy (information entropy)
-> Each split is called leaf


Check the lecture pdf's i cant show the graphs here :-) pycharm effects!

so whenever a new point comes the it checks in which split it falls and then the average of all points is the predicted value

=======================================================================================
Random Forest Regression

Multiple regressions --> Random Forest Regression

step 1 : Pick at random K data points from the training set
step 2 : Build the decision tree associated to those K data points
step 3 : Choose ehte number Ntree of trees you want to build and repeat step 1 and step 2
step 4 : For a new data point make each one of your Ntree trees preict the value of Y to for the
         data point in question and assign the new data point the averafe across all of the predicted Y values

A small change in the data set will not really effect the prediciton because there are so may trees
that the error will get nullified

========
Remember in ML it is thumb rule to have multiple regression models and get a combined predicted values
================================
R- squared --> indicates how well ur model is fitted into the data
            --> Treated as goodness of fit
remember SSres=sum(yi-y'i)^2

now forget the regreesion line we will draw an average line
 and the distance between points squared will be SStot=sum(yi - yavg)^2

 R^2 =1 - SSres/SStot  --SSresidual/SStotal

 R^2 tells how good is ur SSres compared to the average line
more R^2 then good
less R^2 then bad-->change model

========================
Adjusted R-Squared

the shit is with the incsrease in variables R^2 wont decrease
the reason check wikipedia page...quite self explanatory

Adj R^2 = 1 - (1-R^2)* (n-1)/(n-p-1)  here p is number of regressors
                                      n is sample size

                                      n-1/n-p-1 --> is treated as a penalizing factor


==============================

